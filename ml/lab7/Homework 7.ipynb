{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(x):\n",
    "    return float(1) / (1 + math.exp(-x))        \n",
    "\n",
    "\n",
    "def logit(w, x, y, lambd):\n",
    "    return math.log(1 + math.exp(-1*y*np.dot(w, x))) + lambd * math.sqrt(sum([x ** 2 for x in w]))\n",
    "                                                                         \n",
    "                                                                         \n",
    "def logit_loss_partial_deriv(w, samples, j, lambd):\n",
    "    norm = 1\n",
    "    tikhonov_deriv = (lambd * w[j] / norm) if norm > 0 else 0\n",
    "    return np.average([s[-1] * s[j] * (sigmoid(s[-1] * np.dot(w, s[:-1])) - 1) for s in samples]) + tikhonov_deriv\n",
    "\n",
    "\n",
    "def logit_loss_gradient(w, samples, lambd):\n",
    "    d = len(samples[0]) - 1\n",
    "    return [logit_loss_partial_deriv(w, samples, j, lambd) for j in xrange(d)]\n",
    "\n",
    "def logit_loss_one_var_gradient(w, samples, lambd, j):\n",
    "    one_sample = [samples[j]]\n",
    "    return logit_loss_gradient(w, one_sample, lambd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "YES_LABEL = 1\n",
    "NO_LABEL = -1\n",
    "\n",
    "def read_data_from_file(filename):\n",
    "    samples = []\n",
    "    classes = set()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line_splitted = line.strip().split(\",\")\n",
    "            samples.append(([float(x) for x in line_splitted[:-1]], line_splitted[-1]))\n",
    "            classes.add(line_splitted[-1])\n",
    "    return samples, classes\n",
    "\n",
    "\n",
    "def mark_set(samples, class_label):\n",
    "    marked = []\n",
    "    for s in samples:\n",
    "        ts = [1] + list(s[0]) # make homogenous\n",
    "        ts.extend([YES_LABEL if s[1] == class_label else NO_LABEL])\n",
    "        marked.append(ts)\n",
    "    return marked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_gd(samples, T, nu, lambd):\n",
    "    d = len(samples[0]) -1\n",
    "    w = np.zeros(d)\n",
    "    w_history = [w]\n",
    "    \n",
    "    for t in xrange(T):\n",
    "        vt = logit_loss_gradient(w, samples, lambd)\n",
    "        w = np.subtract(w, np.dot(nu, vt))\n",
    "        w_history.append(w)\n",
    "        \n",
    "    w_res = np.zeros(d)\n",
    "    for w_h in w_history:\n",
    "        w_res = np.add(w_res, w_h)\n",
    "        \n",
    "    return w_history[-1]\n",
    "\n",
    "def stochastic_gd(samples, T, nu, lambd):\n",
    "    d = len(samples[0]) - 1\n",
    "    w = np.zeros(d)\n",
    "    w_history = [w]\n",
    "    \n",
    "    for t in xrange(T):\n",
    "        vt = logit_loss_one_var_gradient(w, samples, lambd, random.randint(0, len(samples) - 1))\n",
    "        w = np.subtract(w, np.dot(nu, vt))\n",
    "        w_history.append(w)\n",
    "        \n",
    "    # w_res = np.zeros(d)\n",
    "    # for w_h in w_history:\n",
    "    #     w_res = np.add(w_res, w_h)\n",
    "        \n",
    "    return w_history[-1]\n",
    "\n",
    "\n",
    "def test_sample(x, y, w):\n",
    "    prediction = sigmoid(np.dot(w,x))\n",
    "#     print \"x=\", x, \",y=\", y, \",prediction=\", prediction\n",
    "    if (prediction >= 0.5 and y == YES_LABEL) or (prediction < 0.5 and y == NO_LABEL):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_samples(samples_for_test, w):\n",
    "    predicted = sum( [test_sample(x[:-1], x[-1], w) for x in samples_for_test])\n",
    "    return float(predicted) / len(samples_for_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_RANGE = xrange(100, 101, 3000)\n",
    "LAMBDA_RANGE = np.arange(0.1, 1, 2)\n",
    "NU_RANGE = np.arange(0.1, 1, 2)\n",
    "K = 10\n",
    "\n",
    "def k_fold(samples, k, t_range, lambda_range, nu_range):\n",
    "    block_size = int(math.floor(float(len(samples)) / k))\n",
    "    \n",
    "    T_best = 0\n",
    "    lambda_best = 0\n",
    "    nu_best = 0\n",
    "    predicted_best = 0\n",
    "    T_best_st = 0\n",
    "    lambda_best_st = 0\n",
    "    nu_best_st = 0\n",
    "    predicted_best_st = 0\n",
    "    \n",
    "    for T in t_range:\n",
    "        for l in lambda_range:\n",
    "            for nu in nu_range:\n",
    "                 for ind in xrange(0, len(samples), block_size):\n",
    "                    fold_test = samples[ind:min(ind+block_size, len(samples))]\n",
    "                    fold_train = [x for x in samples if x not in fold_test]\n",
    "                    \n",
    "                    w_fold_trained = batch_gd(fold_train, T, nu, l)\n",
    "                    predicted = test_samples(fold_test, w_fold_trained)\n",
    "                    if predicted > predicted_best:\n",
    "                        T_best = T\n",
    "                        lambda_best = l\n",
    "                        nu_best = nu\n",
    "                        predicted_best = predicted\n",
    "                       \n",
    "                    w_fold_trained_st = stochastic_gd(fold_train, T, nu, l)\n",
    "                    predicted_st = test_samples(fold_test, w_fold_trained)\n",
    "                    if predicted_st > predicted_best_st:\n",
    "                        T_best_st = T\n",
    "                        lambda_best_st = l\n",
    "                        nu_best_st = nu\n",
    "                        predicted_best_st = predicted\n",
    "                    \n",
    "                        \n",
    "                    print \"T=\", T, \",l=\", l, \",nu=\", nu, \",ind=\", ind, \",predicted=\", predicted\n",
    "\n",
    "    \n",
    "    return T_best, lambda_best, nu_best, T_best_st, lambda_best_st, nu_best_st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 0 ,predicted= 0.846153846154\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 13 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 26 ,predicted= 0.769230769231\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 39 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 52 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 65 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 78 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 91 ,predicted= 0.846153846154\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 104 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 117 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 130 ,predicted= 1.0\n",
      "batch: c= Iris-virginica , T= 100 , l= 0.1 , nu= 0.1\n",
      "batch:  [-0.21062496 -0.46112565 -0.48946341  0.74471786  0.53492473] 0.97037037037\n",
      "stochastic: c= Iris-virginica , T= 100 , l= 0.1 , nu= 0.1\n",
      "stochastic:  [-0.22182248 -0.28550741 -0.60407128  1.5137761   1.00224186] 0.659259259259\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 0 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 13 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 26 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 39 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 52 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 65 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 78 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 91 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 104 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 117 ,predicted= 1.0\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 130 ,predicted= 1.0\n",
      "batch: c= Iris-setosa , T= 100 , l= 0.1 , nu= 0.1\n",
      "batch:  [ 0.12553513  0.18292796  0.68782186 -1.11088486 -0.49382388] 1.0\n",
      "stochastic: c= Iris-setosa , T= 100 , l= 0.1 , nu= 0.1\n",
      "stochastic:  [ 0.17401374  0.37270238  0.86019112 -1.17588999 -0.52596047] 0.992592592593\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 0 ,predicted= 0.769230769231\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 13 ,predicted= 0.692307692308\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 26 ,predicted= 0.461538461538\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 39 ,predicted= 0.769230769231\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 52 ,predicted= 0.615384615385\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 65 ,predicted= 0.769230769231\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 78 ,predicted= 0.846153846154\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 91 ,predicted= 0.538461538462\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 104 ,predicted= 0.692307692308\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 117 ,predicted= 0.615384615385\n",
      "T= 100 ,l= 0.1 ,nu= 0.1 ,ind= 130 ,predicted= 1.0\n",
      "batch: c= Iris-versicolor , T= 100 , l= 0.1 , nu= 0.1\n",
      "batch:  [-0.02126654 -0.08240913 -0.29298629  0.16644855 -0.00478276] 0.688888888889\n",
      "stochastic: c= Iris-versicolor , T= 100 , l= 0.1 , nu= 0.1\n",
      "stochastic:  [-0.058392   -0.17658762 -0.7178661   0.5423549   0.10836855] 0.607407407407\n"
     ]
    }
   ],
   "source": [
    "data, classes = read_data_from_file(\"iris.data\")\n",
    "\n",
    "for c in classes:\n",
    "    marked_data = mark_set(data, c)\n",
    "\n",
    "    np.random.shuffle(marked_data)\n",
    "    train_length = int(math.floor(len(marked_data) * 0.9))\n",
    "    train_data = marked_data[:train_length]\n",
    "    test_data = marked_data[train_length:]\n",
    "\n",
    "    T, lambd, nu, T_st, lambd_st, nu_st = k_fold(train_data, K, T_RANGE, LAMBDA_RANGE, NU_RANGE)\n",
    "    \n",
    "    print \"batch: c=\", c, \", T=\", T, \", l=\", lambd, \", nu=\", nu\n",
    "    \n",
    "    w_trained = batch_gd(train_data, T, nu, lambd)\n",
    "    predict = test_samples(test_data, w_trained)\n",
    "    print \"batch: \", w_trained, predict\n",
    "    \n",
    "    print \"stochastic: c=\", c, \", T=\", T_st, \", l=\", lambd_st, \", nu=\", nu_st\n",
    "    \n",
    "    w_trained_st = stochastic_gd(train_data, T, nu, lambd)\n",
    "    predict_st = test_samples(test_data, w_trained_st)\n",
    "    print \"stochastic: \", w_trained_st, predict_st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[ 1  1  1  1 -1  1 -1 -1 -1 -1  1 -1 -1  1 -1]\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X = [x[:-1] for x in train_data]\n",
    "X_test = [x[:-1] for x in test_data]\n",
    "y = [x[-1] for x in train_data]\n",
    "y_test = [x[-1] for x in test_data]\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "clf.fit(X, y)\n",
    "prediction = clf.predict(X_test)\n",
    "original = y_test\n",
    "print prediction\n",
    "print np.array(y_test)\n",
    "print clf\n",
    "\n",
    "predicted = sum([1 if (x[0]*x[1] > 0) else 0 for x in zip(prediction, original)])\n",
    "print float(predicted) / len(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
